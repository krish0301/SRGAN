{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===== 1. Check GPU & Install Dependencies =====\n",
        "#@title **1.1 Check GPU**\n",
        "!nvidia-smi\n",
        "\n",
        "#@title **1.2 Install Python Packages**\n",
        "!pip install torch torchvision pillow scikit-image tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24ok9bjKP8Nq",
        "outputId": "f3949e0a-e1dc-4a5f-e6f2-2bf75b4965d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 2. Download & Prepare DIV2K HR =====\n",
        "\n",
        "#@title **2.1 Download & Flatten HR images**\n",
        "import os, glob, shutil\n",
        "\n",
        "# Cleanup old data\n",
        "!rm -rf data/DIV2K_train_HR data/DIV2K_train_HR.zip temp_DIV2K\n",
        "os.makedirs('data/DIV2K_train_HR', exist_ok=True)\n",
        "\n",
        "# Download DIV2K HR zip\n",
        "!wget -q https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip -O data/DIV2K_train_HR.zip\n",
        "\n",
        "# Unzip into temp and flatten all images into data/DIV2K_train_HR\n",
        "!unzip -q data/DIV2K_train_HR.zip -d temp_DIV2K\n",
        "for pattern in ('*.png','*.jpg','*.jpeg'):\n",
        "    for fp in glob.glob(f\"temp_DIV2K/{pattern}\") + glob.glob(f\"temp_DIV2K/*/{pattern}\"):\n",
        "        shutil.move(fp, 'data/DIV2K_train_HR/')\n",
        "# Cleanup\n",
        "!rm -rf data/DIV2K_train_HR.zip temp_DIV2K\n",
        "\n",
        "# Verify\n",
        "hr_files = glob.glob(\"data/DIV2K_train_HR/*\")\n",
        "print(f\"Found {len(hr_files)} HR images in data/DIV2K_train_HR\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMEPzHEQP8QH",
        "outputId": "82cd254b-423f-4fd0-9430-c13e2b2017c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 HR images in data/DIV2K_train_HR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 3. Define Models =====\n",
        "\n",
        "#@title **3.1 Import Libraries**\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vgg19\n"
      ],
      "metadata": {
        "id": "7YZR5l1yP8Sa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.2 ResidualBlock & Generator**\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, n_feats=64):\n",
        "        super().__init__()\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(n_feats, n_feats, 3, 1, 1),\n",
        "            nn.BatchNorm2d(n_feats),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(n_feats, n_feats, 3, 1, 1),\n",
        "            nn.BatchNorm2d(n_feats),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, n_res_blocks=16, n_feats=64, scale=4):\n",
        "        super().__init__()\n",
        "        self.conv_in = nn.Conv2d(3, n_feats, 9, 1, 4)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.res_blocks = nn.Sequential(*[ResidualBlock(n_feats) for _ in range(n_res_blocks)])\n",
        "        self.conv_mid = nn.Sequential(nn.Conv2d(n_feats, n_feats, 3, 1, 1), nn.BatchNorm2d(n_feats))\n",
        "        upsample = []\n",
        "        for _ in range(int(scale/2)):\n",
        "            upsample += [\n",
        "                nn.Conv2d(n_feats, n_feats*4, 3, 1, 1),\n",
        "                nn.PixelShuffle(2),\n",
        "                nn.PReLU()\n",
        "            ]\n",
        "        self.upsample = nn.Sequential(*upsample)\n",
        "        self.conv_out = nn.Conv2d(n_feats, 3, 9, 1, 4)\n",
        "    def forward(self, x):\n",
        "        x1 = self.prelu(self.conv_in(x))\n",
        "        res = self.res_blocks(x1)\n",
        "        res = self.conv_mid(res)\n",
        "        x2 = x1 + res\n",
        "        out = self.upsample(x2)\n",
        "        return self.conv_out(out)"
      ],
      "metadata": {
        "id": "W1V3XUZQQuZU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.3 Discriminator**\n",
        "def conv_block(in_c, out_c, s):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_c, out_c, 3, s, 1),\n",
        "        nn.BatchNorm2d(out_c),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "    )\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1), nn.LeakyReLU(0.2, inplace=True),\n",
        "            conv_block(64, 64, 2), conv_block(64, 128, 1),\n",
        "            conv_block(128, 128, 2), conv_block(128, 256, 1),\n",
        "            conv_block(256, 256, 2), conv_block(256, 512, 1),\n",
        "            conv_block(512, 512, 2), nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(), nn.Linear(512, 1024), nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1024, 1), nn.Sigmoid(),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "9sXtWPFzQuTm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **3.4 VGG Feature Extractor**\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg19(pretrained=True).features\n",
        "        self.slice = nn.Sequential(*list(vgg)[:36])\n",
        "        for p in self.slice.parameters():\n",
        "            p.requires_grad = False\n",
        "    def forward(self, x):\n",
        "        return self.slice(x)"
      ],
      "metadata": {
        "id": "PWlUHU7sQuMp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 4. Patch-based Dataset & DataLoader =====\n",
        "\n",
        "#@title **4.1 Define SRPatchDataset & DataLoader**\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "class SRPatchDataset(Dataset):\n",
        "    def __init__(self, hr_dir, patch_size=96, scale=4):\n",
        "        super().__init__()\n",
        "        self.hr_paths = sorted(glob.glob(f\"{hr_dir}/*\"))\n",
        "        self.patch_size = patch_size\n",
        "        self.scale = scale\n",
        "        self.to_tensor = ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hr_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        hr = Image.open(self.hr_paths[idx]).convert(\"RGB\")\n",
        "        w, h = hr.size\n",
        "        # ensure random patch fits\n",
        "        ps = self.patch_size\n",
        "        if w < ps or h < ps:\n",
        "            hr = hr.resize((max(ps,w), max(ps,h)), Image.BICUBIC)\n",
        "            w, h = hr.size\n",
        "        left = random.randint(0, w - ps)\n",
        "        top  = random.randint(0, h - ps)\n",
        "        hr_patch = hr.crop((left, top, left + ps, top + ps))\n",
        "        lr_patch = hr_patch.resize((ps // self.scale, ps // self.scale), Image.BICUBIC)\n",
        "        return self.to_tensor(lr_patch), self.to_tensor(hr_patch)\n",
        "\n",
        "# instantiate dataset & loader\n",
        "dataset = SRPatchDataset('data/DIV2K_train_HR', patch_size=96, scale=4)\n",
        "loader  = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "print(f\"Dataset size: {len(dataset)} patches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFGzx6i1P8Ut",
        "outputId": "e0088cfb-fb62-4b33-8d00-791616f76879"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 800 patches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 5. Training Setup =====\n",
        "\n",
        "#@title **5.1 Instantiate Models, Losses & Optimizers**\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "VGG = VGGFeatureExtractor().to(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "mse = nn.MSELoss()\n",
        "bce = nn.BCELoss()\n",
        "optG = optim.Adam(G.parameters(), lr=1e-4)\n",
        "optD = optim.Adam(D.parameters(), lr=1e-4)\n",
        "t_real = lambda n: torch.ones((n,1), device=device)\n",
        "t_fake = lambda n: torch.zeros((n,1), device=device)"
      ],
      "metadata": {
        "id": "aWWIt78FP8Xy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 6. Phase 1: MSE Pre-training =====\n",
        "\n",
        "#@title **6.1 MSE Pre-training Loop**\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "epochs_pre = 10\n",
        "for epoch in range(epochs_pre):\n",
        "    loop = tqdm(loader, desc=f\"Pretrain {epoch+1}/{epochs_pre}\")\n",
        "    for lr_img, hr_img in loop:\n",
        "        lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "        optG.zero_grad()\n",
        "        sr = G(lr_img)\n",
        "        loss = mse(sr, hr_img)\n",
        "        loss.backward()\n",
        "        optG.step()\n",
        "        loop.set_postfix(mse=loss.item())\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "torch.save(G.state_dict(), 'checkpoints/srgan_pretrained.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaCE3P88P8aI",
        "outputId": "2e739447-8b1a-4100-c53e-de5c8a8a6251"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pretrain 1/10: 100%|██████████| 50/50 [04:28<00:00,  5.37s/it, mse=0.0318]\n",
            "Pretrain 2/10: 100%|██████████| 50/50 [04:17<00:00,  5.14s/it, mse=0.0115]\n",
            "Pretrain 3/10: 100%|██████████| 50/50 [04:18<00:00,  5.17s/it, mse=0.00912]\n",
            "Pretrain 4/10: 100%|██████████| 50/50 [04:17<00:00,  5.14s/it, mse=0.00785]\n",
            "Pretrain 5/10: 100%|██████████| 50/50 [04:18<00:00,  5.17s/it, mse=0.00554]\n",
            "Pretrain 6/10: 100%|██████████| 50/50 [04:17<00:00,  5.16s/it, mse=0.00936]\n",
            "Pretrain 7/10: 100%|██████████| 50/50 [04:19<00:00,  5.19s/it, mse=0.00519]\n",
            "Pretrain 8/10: 100%|██████████| 50/50 [04:17<00:00,  5.15s/it, mse=0.00606]\n",
            "Pretrain 9/10: 100%|██████████| 50/50 [04:17<00:00,  5.16s/it, mse=0.00716]\n",
            "Pretrain 10/10: 100%|██████████| 50/50 [04:18<00:00,  5.18s/it, mse=0.00791]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 7. Phase 2: Adversarial Training =====\n",
        "\n",
        "#@title **7.1 GAN Training Loop**\n",
        "epochs_gan = 20\n",
        "for epoch in range(epochs_gan):\n",
        "    loop = tqdm(loader, desc=f\"GAN {epoch+1}/{epochs_gan}\")\n",
        "    for lr_img, hr_img in loop:\n",
        "        lr_img, hr_img = lr_img.to(device), hr_img.to(device)\n",
        "        # Discriminator step\n",
        "        optD.zero_grad()\n",
        "        sr_det = G(lr_img).detach()\n",
        "        lossD = 0.5 * (bce(D(hr_img), t_real(lr_img.size(0))) +\n",
        "                       bce(D(sr_det), t_fake(lr_img.size(0))))\n",
        "        lossD.backward()\n",
        "        optD.step()\n",
        "        # Generator step\n",
        "        optG.zero_grad()\n",
        "        sr = G(lr_img)\n",
        "        content_loss = mse(VGG(sr), VGG(hr_img))\n",
        "        adv_loss     = bce(D(sr), t_real(lr_img.size(0)))\n",
        "        lossG = content_loss + 1e-3 * adv_loss\n",
        "        lossG.backward()\n",
        "        optG.step()\n",
        "        loop.set_postfix(D=lossD.item(), G=lossG.item())\n",
        "    torch.save(G.state_dict(), f'checkpoints/srgan_GAN_epoch{epoch+1}.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut-UWKZ9P8l4",
        "outputId": "4338d54c-36a8-4a12-cf7f-bed67aac918b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GAN 1/20: 100%|██████████| 50/50 [16:09<00:00, 19.39s/it, D=0.0495, G=0.149]\n",
            "GAN 2/20: 100%|██████████| 50/50 [16:09<00:00, 19.39s/it, D=0.0129, G=0.148]\n",
            "GAN 3/20: 100%|██████████| 50/50 [16:11<00:00, 19.43s/it, D=0.0421, G=0.182]\n",
            "GAN 4/20: 100%|██████████| 50/50 [16:10<00:00, 19.42s/it, D=0.0265, G=0.138]\n",
            "GAN 5/20: 100%|██████████| 50/50 [16:09<00:00, 19.40s/it, D=0.0065, G=0.169]\n",
            "GAN 6/20: 100%|██████████| 50/50 [16:21<00:00, 19.63s/it, D=0.013, G=0.156]\n",
            "GAN 7/20: 100%|██████████| 50/50 [16:27<00:00, 19.74s/it, D=0.0077, G=0.149]\n",
            "GAN 8/20: 100%|██████████| 50/50 [16:26<00:00, 19.73s/it, D=0.00407, G=0.205]\n",
            "GAN 9/20: 100%|██████████| 50/50 [16:27<00:00, 19.76s/it, D=0.00785, G=0.148]\n",
            "GAN 10/20: 100%|██████████| 50/50 [16:24<00:00, 19.69s/it, D=0.123, G=0.173]\n",
            "GAN 11/20: 100%|██████████| 50/50 [16:11<00:00, 19.43s/it, D=0.00285, G=0.107]\n",
            "GAN 12/20: 100%|██████████| 50/50 [16:16<00:00, 19.53s/it, D=0.000622, G=0.119]\n",
            "GAN 13/20: 100%|██████████| 50/50 [16:13<00:00, 19.47s/it, D=0.00247, G=0.197]\n",
            "GAN 14/20: 100%|██████████| 50/50 [16:16<00:00, 19.53s/it, D=0.000672, G=0.143]\n",
            "GAN 15/20: 100%|██████████| 50/50 [16:18<00:00, 19.57s/it, D=0.000886, G=0.163]\n",
            "GAN 16/20: 100%|██████████| 50/50 [16:13<00:00, 19.48s/it, D=0.000557, G=0.207]\n",
            "GAN 17/20: 100%|██████████| 50/50 [16:09<00:00, 19.39s/it, D=0.000402, G=0.185]\n",
            "GAN 18/20: 100%|██████████| 50/50 [16:03<00:00, 19.26s/it, D=0.0007, G=0.1]\n",
            "GAN 19/20: 100%|██████████| 50/50 [16:15<00:00, 19.52s/it, D=0.000652, G=0.12]\n",
            "GAN 20/20: 100%|██████████| 50/50 [16:12<00:00, 19.45s/it, D=0.000535, G=0.155]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== 8. Evaluation & Metrics =====\n",
        "#@title **8. Evaluation & Metrics**\n",
        "# 8.1 Identify & Load Checkpoint\n",
        "import os, re\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "ckpts = os.listdir('checkpoints')\n",
        "print(\"Available checkpoints:\", ckpts)\n",
        "\n",
        "gan_ckpts = [f for f in ckpts if 'srgan_GAN_epoch' in f]\n",
        "if gan_ckpts:\n",
        "    # pick highest‐numbered epoch\n",
        "    epochs = {int(re.search(r'epoch(\\d+)', f).group(1)): f for f in gan_ckpts}\n",
        "    best = epochs[max(epochs)]\n",
        "    ckpt_path = os.path.join('checkpoints', best)\n",
        "else:\n",
        "    ckpt_path = os.path.join('checkpoints', 'srgan_pretrained.pth')\n",
        "\n",
        "print(\"Loading checkpoint:\", ckpt_path)\n",
        "G.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "G.eval()\n",
        "to_pil = ToPILImage()\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "# 8.2 Super-Resolve Sample Patches\n",
        "for i in range(10):\n",
        "    lr, _ = dataset[i]\n",
        "    with torch.no_grad():\n",
        "        sr = G(lr.unsqueeze(0).to(device))\n",
        "    to_pil(sr.squeeze(0).cpu()).save(f'results/sample_{i}.png')\n",
        "print(\"Saved samples to results/\")\n",
        "\n",
        "# 8.3 Compute PSNR & SSIM (with a fallback for small images)\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "import skimage.color as sc\n",
        "\n",
        "psnr_vals, ssim_vals = [], []\n",
        "for i in range(10):\n",
        "    # load SR and HR\n",
        "    sr = np.array(Image.open(f'results/sample_{i}.png')) / 255.0\n",
        "    _, hr_tensor = dataset[i]\n",
        "    hr = hr_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # PSNR\n",
        "    psnr_vals.append(peak_signal_noise_ratio(hr, sr, data_range=1.0))\n",
        "\n",
        "    # SSIM—prefer color, but fallback to grayscale if window too large\n",
        "    try:\n",
        "        ssim_vals.append(structural_similarity(hr, sr,\n",
        "                                               channel_axis=2,\n",
        "                                               data_range=1.0))\n",
        "    except ValueError:\n",
        "        hr_gray = sc.rgb2gray(hr)\n",
        "        sr_gray = sc.rgb2gray(sr)\n",
        "        ssim_vals.append(structural_similarity(hr_gray,\n",
        "                                               sr_gray,\n",
        "                                               data_range=1.0))\n",
        "\n",
        "print(f\"Avg PSNR: {np.mean(psnr_vals):.2f}, Avg SSIM: {np.mean(ssim_vals):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttDIY3lWQbo7",
        "outputId": "b4d08d0f-732c-4561-c40e-285db8b6ffee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available checkpoints: ['srgan_GAN_epoch19.pth', 'srgan_GAN_epoch5.pth', 'srgan_GAN_epoch6.pth', 'srgan_GAN_epoch2.pth', 'srgan_GAN_epoch7.pth', 'srgan_GAN_epoch15.pth', 'srgan_GAN_epoch12.pth', 'srgan_GAN_epoch17.pth', 'srgan_GAN_epoch8.pth', 'srgan_GAN_epoch20.pth', 'srgan_GAN_epoch9.pth', 'srgan_GAN_epoch16.pth', 'srgan_GAN_epoch11.pth', 'srgan_GAN_epoch3.pth', 'srgan_GAN_epoch1.pth', 'srgan_pretrained.pth', 'srgan_GAN_epoch14.pth', 'srgan_GAN_epoch18.pth', 'srgan_GAN_epoch13.pth', 'srgan_GAN_epoch10.pth', 'srgan_GAN_epoch4.pth']\n",
            "Loading checkpoint: checkpoints/srgan_GAN_epoch20.pth\n",
            "Saved samples to results/\n",
            "Avg PSNR: 6.76, Avg SSIM: 0.0334\n"
          ]
        }
      ]
    }
  ]
}